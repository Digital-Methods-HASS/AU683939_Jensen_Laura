---
title: "GoT"
output: html_document
date: "2022-12-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

First I load the document into my Rmarkdown document:
```{r get-document}
got_path <- here("data","got.pdf")
got_text <- pdf_text(got_path)
```

Then I split all the text into separate lines by splitting at newline by using the regular expression "\\n" and trim all spaces at the end of the line:
```{r split-lines}
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 
```

Then I split the all the lines further into individual words:
```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
```

Then I count all the words and sort them by the most frequent word:
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```

Then I apply a stop wordlist to the wordlist
```{r stopwords}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
```

Then I count the words again and sort them by the most frequent, after appling the stop wordlist
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
```

Then I exclude all numbers: 
```{r skip-numbers}
got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
```

Then I have to make a wordcloud. In an attempt at making a comprehensible wordcloud I limit it to only the 100 most frequent words:
```{r word-cloud-prep}
length(unique(got_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

Then I make the wordcloud:
```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud
```

It looks a little boring, and doesn't give a lot of information. Therefore I add some things to the code of the wordcloud to make it nicer to look at:
```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("#72286F","darkred","#CF2129")) +
  theme_minimal()
```

To make the sentimant analysis, I first load the different lexicons:
```{r lexicons}
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "nrc")
```
I first bind the words in `got_stop` to the `afinn` lexicon
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Then I count the words by sentiment value, and plot them, so I can see how many words there are of each value:
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_bar(stat = "identity", color = "darkblue", fill = "#1A5797") + 
  theme_light()
```

Now I'm going to investigate the words with the sentiment value of -5. I filter them out, and then I count and plot them 
```{r afinn--5}
got_afinn5 <- got_afinn %>% 
  filter(value == -5)

unique(got_afinn5$word)

# Count & plot them
got_afinn5_n <- got_afinn5 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn5_n, aes(x = word, y = n)) +
  geom_bar(stat = "identity", color = "darkblue", fill = "#1A5797") + 
  theme_dark() +
  coord_flip()
```

Then I summarize the sentimant value of the words in the text, and find that the sentimant over all are more negative, than positive:
```{r summarixe-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

As above, we'll use inner_join() to combine the GoT non-stopword text with the nrc lexicon:
```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

By doing that we exclude some of the words from the text, so now we have to have a look at the excluded words:
```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(ipcc_exclude)

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```

Now I count the words by sentiment and plot them:
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_bar(stat = "identity", color = "darkblue", fill = "#1A5797") + 
  theme_light()
```

I also try to count them by sentiment and word, and then facet them. I that way I'll be able to see the most frequent words in each sentiment. I have choosen to see the five most frequent words under each sentiment:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("figures","ipcc_nrc_sentiment.png"), 
       height = 8, 
       width = 5)
```

